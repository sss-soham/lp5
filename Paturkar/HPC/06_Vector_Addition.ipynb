{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef45c25",
   "metadata": {},
   "source": [
    "Problem Statment: Write a CUDA Program for Addition of two large vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e8ab9",
   "metadata": {},
   "source": [
    "-- Program Overview --\n",
    "\n",
    "This program performs vector addition using CUDA (Compute Unified Device Architecture) to run parallel code on an NVIDIA GPU. It:\n",
    "    Generates two large vectors A and B with random integers.\n",
    "    Adds the vectors element-wise in parallel on the GPU.\n",
    "    Stores the result in vector C.\n",
    "    Writes all three vectors to a text file in tabular format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952220d",
   "metadata": {},
   "source": [
    "1. CUDA Basics\n",
    "    CUDA is a parallel computing platform by NVIDIA that allows using the GPU (Graphics Processing Unit) for general-purpose computing.\n",
    "    It uses extensions to C/C++ to run functions (called kernels) in parallel across many GPU threads.\n",
    "\n",
    "2. Kernels\n",
    "    A kernel is a function that runs on the GPU. It’s called with <<<blocks, threads>>> syntax.\n",
    "    Each thread computes a part of the problem—in this case, one index of the vector sum.\n",
    "\n",
    "3. Memory Management\n",
    "    Host (CPU) and Device (GPU) have separate memory spaces.\n",
    "    You must explicitly allocate GPU memory (cudaMalloc) and copy data between host and device (cudaMemcpy).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee69b6",
   "metadata": {},
   "source": [
    "Explanation of How the Program Works\n",
    "\n",
    "Step 1: Initialization\n",
    "- N = 2^20 = 1,048,576 defines the size of the vectors.\n",
    "- Host vectors h_A, h_B, h_C are created using std::vector.\n",
    "- Random values (0–100) are assigned to h_A and h_B.\n",
    "\n",
    "Step 2: GPU Memory Allocation\n",
    "- cudaMalloc allocates memory for d_A, d_B, d_C on the GPU.\n",
    "- cudaMemcpy copies vectors h_A and h_B to the GPU.\n",
    "\n",
    "Step 3: Kernel Launch\n",
    "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
    "\n",
    "- Launches the GPU kernel vectorAdd.\n",
    "- Each thread computes: C[i] = A[i] + B[i].\n",
    "\n",
    "Step 4: Error Checking & Synchronization\n",
    "- cudaGetLastError() checks if kernel launch was successful.\n",
    "- cudaDeviceSynchronize() ensures the kernel execution is finished.\n",
    "\n",
    "Step 5: Copy Back Result\n",
    "- cudaMemcpy copies result d_C to host vector h_C.\n",
    "\n",
    "Step 6: Output to File\n",
    "- The result is saved to vector_sum_output.txt in a well-formatted table using std::ofstream.\n",
    "\n",
    "Step 7: Memory Cleanup\n",
    "- Frees GPU memory using cudaFree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807fe70",
   "metadata": {},
   "source": [
    "Important CUDA Functions Used\n",
    "\n",
    "1. cudaMalloc:\tAllocates memory on GPU\n",
    "2. cudaMemcpy:\tCopies data between CPU and GPU\n",
    "3. cudaFree:\tFrees allocated GPU memory\n",
    "4. cudaGetLastError:\tChecks for kernel launch errors\n",
    "5. cudaDeviceSynchronize:\tWaits for all GPU threads to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e71ab8",
   "metadata": {},
   "source": [
    "The below code is for the vector addition, which we have to perform on the Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4a22ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = r'''\n",
    "#include <iostream>\n",
    "#include <fstream>\n",
    "#include <vector>\n",
    "#include <cstdlib>\n",
    "#include <ctime>\n",
    "#include <iomanip>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void vectorAdd(const int *A, const int *B, int *C, int N) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < N) {\n",
    "        C[idx] = A[idx] + B[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "void checkCudaError(cudaError_t err, const char *msg) {\n",
    "    if (err != cudaSuccess) {\n",
    "        std::cerr << \"CUDA Error: \" << msg << \": \" << cudaGetErrorString(err) << std::endl;\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1 << 20;\n",
    "    size_t size = N * sizeof(int);\n",
    "\n",
    "    std::vector<int> h_A(N), h_B(N), h_C(N);\n",
    "\n",
    "    srand(static_cast<unsigned>(time(nullptr)));\n",
    "    for (int i = 0; i < N; ++i) {\n",
    "        h_A[i] = rand() % 101;  // integers from 0 to 100\n",
    "        h_B[i] = rand() % 101;\n",
    "    }\n",
    "\n",
    "    int *d_A = nullptr, *d_B = nullptr, *d_C = nullptr;\n",
    "    checkCudaError(cudaMalloc(&d_A, size), \"Allocating d_A\");\n",
    "    checkCudaError(cudaMalloc(&d_B, size), \"Allocating d_B\");\n",
    "    checkCudaError(cudaMalloc(&d_C, size), \"Allocating d_C\");\n",
    "\n",
    "    checkCudaError(cudaMemcpy(d_A, h_A.data(), size, cudaMemcpyHostToDevice), \"Copying h_A\");\n",
    "    checkCudaError(cudaMemcpy(d_B, h_B.data(), size, cudaMemcpyHostToDevice), \"Copying h_B\");\n",
    "\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
    "\n",
    "    checkCudaError(cudaGetLastError(), \"Kernel launch\");\n",
    "    checkCudaError(cudaDeviceSynchronize(), \"Kernel execution\");\n",
    "\n",
    "    checkCudaError(cudaMemcpy(h_C.data(), d_C, size, cudaMemcpyDeviceToHost), \"Copying result\");\n",
    "\n",
    "    // Save to file in column format\n",
    "    std::ofstream outFile(\"vector_sum_output.txt\");\n",
    "    if (!outFile.is_open()) {\n",
    "        std::cerr << \"Error opening output file!\" << std::endl;\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    outFile << std::setw(10) << \"A[i]\"\n",
    "            << std::setw(10) << \"B[i]\"\n",
    "            << std::setw(15) << \"C[i] = A + B\" << \"\\n\";\n",
    "    outFile << std::string(35, '-') << \"\\n\";\n",
    "\n",
    "    for (int i = 0; i < N; ++i) {\n",
    "        outFile << std::setw(10) << h_A[i]\n",
    "                << std::setw(10) << h_B[i]\n",
    "                << std::setw(15) << h_C[i] << \"\\n\";\n",
    "    }\n",
    "\n",
    "    outFile.close();\n",
    "\n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "'''\n",
    "with open(\"vector_add_random.cu\", \"w\") as f:\n",
    "    f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "024ac7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvcc' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'.' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -arch=sm_75 vector_add_random.cu -o vector_add_random\n",
    "!./vector_add_random"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
